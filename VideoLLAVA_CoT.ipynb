{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98e64bf9-4868-41b4-8bd5-8288b516aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56109d3a-2d44-4e6d-8de6-15f71270ed8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yid016/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adagrad\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import bisect\n",
    "import shutil\n",
    "import json\n",
    "from time import perf_counter\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import av\n",
    "from transformers import VideoLlavaForConditionalGeneration, VideoLlavaProcessor\n",
    "from huggingface_hub import hf_hub_download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d24446f-fa00-459c-9eeb-e8a850e904f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████| 3/3 [00:31<00:00, 10.33s/it]\n"
     ]
    }
   ],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "from transformers import VideoLlavaProcessor, VideoLlavaForConditionalGeneration\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "model = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\")\n",
    "processor = VideoLlavaProcessor.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d30b7526-d4be-41be-9288-4eb376805ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_path = \"data/Charades_v1_480/6H78U.mp4\"\n",
    "video_path = \"data/Charades_v1_480/0A8CF.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e7fed-5896-42a1-ac55-42b00e7e14f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in Video-LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.44.\n",
      "Expanding inputs for image tokens in Video-LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"USER: <video>Why is this video funny? ASSISTANT:\"\n",
    "\n",
    "container = av.open(video_path)\n",
    "\n",
    "# sample uniformly 8 frames from the video\n",
    "total_frames = container.streams.video[0].frames\n",
    "indices = np.arange(0, total_frames, total_frames / 8).astype(int)\n",
    "clip = read_video_pyav(container, indices)\n",
    "\n",
    "inputs = processor(text=prompt, videos=clip, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(**inputs, max_length=80)\n",
    "print(processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad65090c-814f-4c06-aa41-70bfe835ab8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c9fa64f2-14a1-4ac5-9c5e-55a6c874b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoQADataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        json_file,\n",
    "        video_dir=\"/data/user_data/gdhanuka/STAR_dataset/Charades_v1_480\",\n",
    "        sampling_fps=4,\n",
    "        num_frames=8,\n",
    "        use_fps=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            json_file (str): Path to the JSON file containing the dataset.\n",
    "            video_features_dir (str): Directory containing precomputed CLIP features for videos.\n",
    "            num_frames (int): Number of frames to sample from each video.\n",
    "        \"\"\"\n",
    "        with open(json_file, \"rb\") as f:\n",
    "            self.data = pickle.load(f)\n",
    "        self.video_dir = video_dir\n",
    "        self.sampling_fps = sampling_fps\n",
    "        self.num_frames = num_frames\n",
    "        self.use_fps = use_fps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def read_video_pyav(self, container, indices):\n",
    "        \"\"\"\n",
    "        Decode the video with PyAV decoder.\n",
    "        Args:\n",
    "            container (`av.container.input.InputContainer`): PyAV container.\n",
    "            indices (`List[int]`): List of frame indices to decode.\n",
    "        Returns:\n",
    "            result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "        \"\"\"\n",
    "        frames = []\n",
    "        container.seek(0)\n",
    "        start_index = indices[0]\n",
    "        end_index = indices[-1]\n",
    "        for i, frame in enumerate(container.decode(video=0)):\n",
    "            if i > end_index:\n",
    "                break\n",
    "            if i >= start_index and i in indices:\n",
    "                frames.append(frame)\n",
    "        return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "    def read_video_pyav2(self, video_path, start, end, num_frames=8):\n",
    "        \"\"\"Reads a video for given start-end timestamps interval and uniformly samples 8 frames of it\"\"\"\n",
    "        container = av.open(video_path)\n",
    "        video = container.streams.get(0)[0]\n",
    "\n",
    "        av_timestamps = [\n",
    "            int(packet.pts * video.time_base)\n",
    "            for packet in container.demux(video)\n",
    "            if packet.pts is not None\n",
    "        ]\n",
    "\n",
    "        av_timestamps.sort()\n",
    "        start_id = bisect.bisect_left(av_timestamps, start)\n",
    "        end_id = bisect.bisect_left(av_timestamps, end)\n",
    "\n",
    "        # in case it is a very short video, lets take a longer duration and sample\n",
    "        if end_id - start_id < 10:\n",
    "            end_id += 10\n",
    "            start_id -= 10\n",
    "\n",
    "        end_id = min(len(av_timestamps) - 1, end_id)\n",
    "        start_id = max(1, start_id)\n",
    "\n",
    "        # We sample 8 frames for tuning following the original paper\n",
    "        # But we can increase the number of frames for longer videos and check out if it helps performance\n",
    "        # Change the below \"8\" to any number of frames you want, and note that more frames -> more computational resources needed\n",
    "        indices = np.linspace(start_id, end_id, num_frames).astype(int)\n",
    "\n",
    "        frames = []\n",
    "        container.seek(0)\n",
    "        for i, frame in enumerate(container.decode(video=0)):\n",
    "            if i > end_id:\n",
    "                break\n",
    "            if i >= start_id and i in indices:\n",
    "                frames.append(frame)\n",
    "        assert (\n",
    "            len(frames) == num_frames\n",
    "        ), f\"Got {len(frames)} frames but should be {num_frames}. Check the indices: {indices};, start_id: {start_id}, end_id: {end_id}. Len of video is {len(av_timestamps)} frames.\"\n",
    "        return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames]), indices\n",
    "\n",
    "    def read_video_pyav3(self, video_path, start, end, sampling_fps=4):\n",
    "        \"\"\"Reads a video clip from start-end timestamps and samples frames at specified FPS\"\"\"\n",
    "        container = av.open(video_path)\n",
    "        video = container.streams.video[0]\n",
    "        \n",
    "        # Calculate number of frames needed based on duration and sampling FPS\n",
    "        duration = end - start\n",
    "        num_frames = int(round(sampling_fps * duration))\n",
    "        num_frames = max(1, num_frames)  # Ensure at least 1 frame\n",
    "\n",
    "        # Get sorted presentation timestamps\n",
    "        av_timestamps = [\n",
    "            int(packet.pts * video.time_base)\n",
    "            for packet in container.demux(video)\n",
    "            if packet.pts is not None\n",
    "        ]\n",
    "        av_timestamps.sort()\n",
    "\n",
    "        # Find frame indices bounding our clip\n",
    "        start_id = bisect.bisect_left(av_timestamps, start)\n",
    "        end_id = bisect.bisect_left(av_timestamps, end)\n",
    "\n",
    "        # Expand window for short clips\n",
    "        if end_id - start_id < 10:\n",
    "            end_id += 10\n",
    "            start_id -= 10\n",
    "\n",
    "        # Clamp to valid range\n",
    "        end_id = min(len(av_timestamps) - 1, end_id)\n",
    "        start_id = max(0, start_id)\n",
    "\n",
    "        # Generate sampling indices\n",
    "        indices = np.linspace(start_id, end_id, num_frames, dtype=int)\n",
    "\n",
    "        # Extract frames\n",
    "        frames = []\n",
    "        container.seek(0)\n",
    "        for i, frame in enumerate(container.decode(video=0)):\n",
    "            if i > end_id:\n",
    "                break\n",
    "            if i >= start_id and i in indices:\n",
    "                frames.append(frame)\n",
    "        \n",
    "        assert len(frames) == num_frames, (\n",
    "            f\"Frame sampling failed: Expected {num_frames}, got {len(frames)}. \"\n",
    "            f\"Time range: {start}-{end}s ({duration}s), Sampling FPS: {sampling_fps}.\"\n",
    "        )\n",
    "        \n",
    "        return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames]), indices\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        item = self.data[idx]\n",
    "        video_id = item[\"video_id\"]\n",
    "        start, end = item[\"start\"], item[\"end\"]\n",
    "        question = item[\"question\"]\n",
    "        question_id = item[\"question_id\"]\n",
    "        choices = [choice[\"choice\"] for choice in item[\"choices\"]]\n",
    "        answer_idx = next(\n",
    "            i\n",
    "            for i, choice in enumerate(item[\"choices\"])\n",
    "            if choice[\"choice\"] == item[\"answer\"]\n",
    "        )\n",
    "\n",
    "        video_path = os.path.join(self.video_dir, f\"{video_id}.mp4\")\n",
    "        start_time = perf_counter()\n",
    "        if self.use_fps:\n",
    "            video_frames, frame_idx = self.read_video_pyav3(video_path, start, end, sampling_fps=self.sampling_fps)\n",
    "        else:\n",
    "            video_frames, frame_idx = self.read_video_pyav2(video_path, start, end, num_frames=self.num_frames)\n",
    "        video_frames = torch.from_numpy(video_frames).permute(0, 3, 1, 2).float() # (#frames, channel, h, w)\n",
    "\n",
    "        all_text_inputs = []\n",
    "        for choice in choices:\n",
    "            all_text_inputs.append(f\"{question} [SEP] {choice}\")\n",
    "        end = perf_counter()\n",
    "        return {\n",
    "            \"video_frames\": video_frames,  # Video features\n",
    "            \"question\": question,\n",
    "            \"video_id\": video_id,\n",
    "            \"choices\": choices,\n",
    "            \"answer_idx\": answer_idx,\n",
    "            \"category\": item[\"question_id\"].split(\"_\")[0],  # Question category\n",
    "            \"all_text_inputs\": all_text_inputs,\n",
    "            \"data_proc_time\": end-start,\n",
    "            \"question_id\": question_id,\n",
    "            \"frame_ids\": frame_idx\n",
    "        }\n",
    "\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     \"\"\"Handles variable-sized video frames using smart padding\"\"\"\n",
    "#     # Separate video frames and metadata\n",
    "#     videos = [item[\"video_frames\"] for item in batch]\n",
    "#     questions = [item[\"question\"] for item in batch]\n",
    "#     choices = [item[\"choices\"] for item in batch]\n",
    "#     answer_idxs = torch.stack([torch.tensor(item[\"answer_idx\"]) for item in batch])\n",
    "\n",
    "#     # Pad videos to max dimensions in batch\n",
    "#     max_frames = max(vid.shape[0] for vid in videos)\n",
    "#     max_height = max(vid.shape[2] for vid in videos)\n",
    "#     max_width = max(vid.shape[3] for vid in videos)\n",
    "\n",
    "#     padded_videos = []\n",
    "#     for vid in videos:\n",
    "#         # Pad: (width_left, width_right, height_top, height_bottom, frames_front, frames_back)\n",
    "#         pad_width = max_width - vid.shape[3]\n",
    "#         pad_height = max_height - vid.shape[2]\n",
    "#         pad_frames = max_frames - vid.shape[0]\n",
    "\n",
    "#         padded = F.pad(vid, (0, pad_width, 0, pad_height, 0, 0, 0, pad_frames))\n",
    "#         padded_videos.append(padded)\n",
    "\n",
    "#     return {\n",
    "#         \"video_frames\": torch.stack(padded_videos),\n",
    "#         \"question\": questions,\n",
    "#         \"choices\": choices,\n",
    "#         \"answer_idx\": answer_idxs,\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "02192bf8-2f02-4d65-b181-98e23c22956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoQAModel:\n",
    "    def __init__(self, load_in_bits=16):\n",
    "\n",
    "        device = 'cuda'\n",
    "        compute_dtype = 'fp16'\n",
    "        double_quant = True\n",
    "        quant_type = 'nf4'\n",
    "\n",
    "        compute_dtype = (torch.float16 if compute_dtype == 'fp16' else (torch.bfloat16 if compute_dtype == 'bf16' else torch.float32))\n",
    "        \n",
    "        bnb_model_from_pretrained_args = {}\n",
    "        if load_in_bits in [4, 8]:\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            bnb_model_from_pretrained_args.update(dict(\n",
    "                device_map={\"\": device},\n",
    "                # load_in_4bit=load_in_bits == 4,\n",
    "                # load_in_8bit=load_in_bits == 8,\n",
    "                quantization_config=BitsAndBytesConfig(\n",
    "                    load_in_4bit=load_in_bits == 4,\n",
    "                    load_in_8bit=load_in_bits == 8,\n",
    "                    llm_int8_skip_modules=[\"mm_projector\"],\n",
    "                    llm_int8_threshold=6.0,\n",
    "                    llm_int8_has_fp16_weight=False,\n",
    "                    bnb_4bit_compute_dtype=compute_dtype,\n",
    "                    bnb_4bit_use_double_quant=double_quant,\n",
    "                    bnb_4bit_quant_type=quant_type # {'fp4', 'nf4'}\n",
    "                )\n",
    "            ))\n",
    "\n",
    "            self.model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "                \"LanguageBind/Video-LLaVA-7B-hf\",\n",
    "                torch_dtype=compute_dtype,\n",
    "                attn_implementation=\"flash_attention_2\",\n",
    "                **bnb_model_from_pretrained_args\n",
    "            )\n",
    "        else:\n",
    "            self.model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "                \"LanguageBind/Video-LLaVA-7B-hf\",\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                attn_implementation=\"flash_attention_2\",\n",
    "            ).to(\"cuda\")\n",
    "        self.processor = VideoLlavaProcessor.from_pretrained(\n",
    "            \"LanguageBind/Video-LLaVA-7B-hf\"\n",
    "        )\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens=500):\n",
    "        outputs = self.model.generate(\n",
    "            **inputs, max_new_tokens=max_new_tokens,\n",
    "            return_dict_in_generate=True, output_scores=True\n",
    "        )\n",
    "\n",
    "        decoded = self.processor.batch_decode(\n",
    "            outputs.sequences,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False\n",
    "        )\n",
    "\n",
    "        print(decoded)\n",
    "        \n",
    "        first_token_probs = torch.nn.functional.softmax(outputs.scores[0], dim=-1)\n",
    "        \n",
    "        # Get token IDs for numbers 1-4\n",
    "        token_ids = [self.processor.tokenizer.convert_tokens_to_ids(str(i)) for i in [1,2,3,4]]\n",
    "        \n",
    "        # Create probability dictionary for each sample in batch\n",
    "        prob_list = []\n",
    "        for batch_idx in range(first_token_probs.shape[0]):\n",
    "            probs = [\n",
    "                first_token_probs[batch_idx, token_ids[i]].item()\n",
    "                for i in range(4)\n",
    "            ]\n",
    "            prob_list.append(probs)\n",
    "        \n",
    "        logits_list = []\n",
    "        for batch_idx in range(outputs.scores[0].shape[0]):\n",
    "            logits = [\n",
    "                outputs.scores[0][batch_idx, token_ids[i]].item()\n",
    "                for i in range(4)\n",
    "            ]\n",
    "            logits_list.append(logits)\n",
    "\n",
    "\n",
    "        return decoded, prob_list, logits_list\n",
    "\n",
    "    def video_qa(self, video_frames, question, choices, max_new_tokens=500):\n",
    "        choice_with_idx = [f'\"{i+1}\": {choice}\\n' for i, choice in enumerate(choices)]\n",
    "        prompt = f\"USER: <video>\\n {question} \\n {choice_with_idx} Answer with the option's index from the given choices directly. \\n ASSISTANT: \"\n",
    "        inputs = self.processor(\n",
    "            text=prompt, videos=video_frames, return_tensors=\"pt\", max_length=4096\n",
    "        ).to(\"cuda\")\n",
    "        decoded, probs, logits = self.generate(inputs, max_new_tokens=max_new_tokens)\n",
    "        return decoded[0], probs[0], prompt, logits[0]\n",
    "\n",
    "    def video_qa_batch(self, video_batch, questions, choices_batch):\n",
    "        prompts = []\n",
    "        for q, choices in zip(questions, choices_batch):\n",
    "            opts = \"\\n\".join([f\"{i+1}: {c}\" for i, c in enumerate(choices)])\n",
    "            prompts.append(\n",
    "                f\"USER: <video>\\n According to the video choose the correct answer, {questions} \\n {opts} ASSISTANT: \"\n",
    "            )\n",
    "\n",
    "        inputs = self.processor(\n",
    "            text=prompts,\n",
    "            videos=[v for v in video_batch],  # Process full batch\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "        ).to(\"cuda\", torch.float16)\n",
    "\n",
    "        outputs = self.model.generate(**inputs, max_new_tokens=20)\n",
    "        return self.processor.batch_decode(outputs, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "013df251-d9de-4c21-ab0d-f25f5bcf4d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "import numpy as np\n",
    "from time import perf_counter\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "class VideoOfThoughtPredictor:\n",
    "    def __init__(self, video_llava_model):\n",
    "        \"\"\"\n",
    "        Initialize the Video-of-Thought predictor with a VideoLLAVA model.\n",
    "        \n",
    "        Args:\n",
    "            video_llava_model: An instance of the VideoLLAVA model class\n",
    "        \"\"\"\n",
    "        self.model = video_llava_model\n",
    "        \n",
    "    def _generate_response(self, video_frames, prompt, max_new_tokens=100):\n",
    "        \"\"\"\n",
    "        Generate a response from the VideoLLAVA model.\n",
    "        \n",
    "        Args:\n",
    "            video_frames: Video frame tensors\n",
    "            prompt: Text prompt\n",
    "            max_new_tokens: Maximum number of tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            Generated text response\n",
    "        \"\"\"\n",
    "        inputs = self.model.processor(\n",
    "            text=prompt, \n",
    "            videos=video_frames, \n",
    "            return_tensors=\"pt\", \n",
    "            max_length=4096\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        # Use more controlled generation parameters\n",
    "        outputs = self.model.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            # do_sample=False,  # Use greedy decoding for more consistent outputs\n",
    "            # temperature=0.1,   # Lower temperature for more focused responses\n",
    "            # num_beams=1,\n",
    "            # early_stopping=True,\n",
    "            # pad_token_id=self.model.processor.tokenizer.pad_token_id,\n",
    "            # eos_token_id=self.model.processor.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        print(outputs)\n",
    "        \n",
    "        decoded = self.model.processor.batch_decode(\n",
    "            outputs,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        \n",
    "        # Extract just the assistant's response, removing the prompt\n",
    "        response = decoded[0]\n",
    "        if \"ASSISTANT:\" in response:\n",
    "            response = response.split(\"ASSISTANT:\")[-1].strip()\n",
    "            \n",
    "        return response\n",
    "    \n",
    "    def step_1_identify_targets(self, video_frames, question, is_multi_choice=True):\n",
    "        \"\"\"\n",
    "        Step 1: Task Definition and Target Identification\n",
    "        \n",
    "        Args:\n",
    "            video_frames: Video frame tensors\n",
    "            question: The question text\n",
    "            is_multi_choice: Whether the question is multiple choice\n",
    "            \n",
    "        Returns:\n",
    "            The identified targets in the video relevant to the question\n",
    "        \"\"\"\n",
    "        if is_multi_choice:\n",
    "            task_definition = \"You are an expert in video analysis.\"\n",
    "        else:\n",
    "            task_definition = \"You are an expert in video analysis.\"\n",
    "        \n",
    "        prompt = f\"USER: <video>\\n{task_definition}\\n\\nGiven the question: \\\"{question}\\\", what are the key objects, people, or elements in the video that need to be tracked to answer this question?\\n\\nProvide a concise list of the key targets.\\nASSISTANT:\"\n",
    "        \n",
    "        response = self._generate_response(video_frames, prompt, max_new_tokens=100)\n",
    "        return response\n",
    "    \n",
    "    def step_2_object_description(self, video_frames, targets, question):\n",
    "        \"\"\"\n",
    "        Step 2: Object Description (adapted from Object Tracking in the original paper)\n",
    "        \n",
    "        Args:\n",
    "            video_frames: Video frame tensors\n",
    "            targets: The identified targets from step 1\n",
    "            question: The original question\n",
    "            \n",
    "        Returns:\n",
    "            Description of the targets throughout the video\n",
    "        \"\"\"\n",
    "        prompt = f\"USER: <video>\\nDescribe in detail the following elements that are relevant to answering the question \\\"{question}\\\":\\n\\n{targets}\\n\\nFocus on their appearance, movement, and interactions in the video.\\nASSISTANT:\"\n",
    "        \n",
    "        response = self._generate_response(video_frames, prompt, max_new_tokens=150)\n",
    "        return response\n",
    "    \n",
    "    def step_3_action_analysis(self, video_frames, object_descriptions, question):\n",
    "        \"\"\"\n",
    "        Step 3: Action Analysis\n",
    "        \n",
    "        Args:\n",
    "            video_frames: Video frame tensors\n",
    "            object_descriptions: The object descriptions from step 2\n",
    "            question: The original question\n",
    "            \n",
    "        Returns:\n",
    "            Analysis of actions and implications\n",
    "        \"\"\"\n",
    "        prompt = f\"USER: <video>\\nBased on the question \\\"{question}\\\" and these observations:\\n\\n{object_descriptions}\\n\\nAnalyze what actions are occurring in the video, their sequence, and their implications. Include both direct observations and reasonable inferences.\\nASSISTANT:\"\n",
    "        \n",
    "        response = self._generate_response(video_frames, prompt, max_new_tokens=200)\n",
    "        return response\n",
    "    \n",
    "    def step_4_answer_scoring(self, video_frames, question, choices, action_analysis):\n",
    "        \"\"\"\n",
    "        Step 4: Answer Scoring and Ranking for multi-choice questions\n",
    "        \n",
    "        Args:\n",
    "            video_frames: Video frame tensors\n",
    "            question: The question text\n",
    "            choices: List of answer choices\n",
    "            action_analysis: The action analysis from step 3\n",
    "            \n",
    "        Returns:\n",
    "            Final answer with scores\n",
    "        \"\"\"\n",
    "        # First, score each choice individually\n",
    "        scores_and_rationales = []\n",
    "        \n",
    "        for i, choice in enumerate(choices):\n",
    "            prompt = f\"USER: <video>\\nQuestion: {question}\\nCandidate answer: {choice}\\n\\nBased on the video and this analysis:\\n{action_analysis}\\n\\nRate the likelihood of this answer being correct (1-10) and explain why.\\nASSISTANT:\"\n",
    "            \n",
    "            response = self._generate_response(video_frames, prompt, max_new_tokens=150)\n",
    "            scores_and_rationales.append(response)\n",
    "        \n",
    "        # Now do the final ranking and selection\n",
    "        prompt = f\"USER: <video>\\nFor the question: \\\"{question}\\\", here are the ratings for each answer choice:\\n\\n\"\n",
    "        \n",
    "        for i, (choice, rationale) in enumerate(zip(choices, scores_and_rationales)):\n",
    "            prompt += f\"Option {i+1}: {choice}\\nRating: {rationale}\\n\\n\"\n",
    "        \n",
    "        prompt += \"Based on these ratings, which answer is most likely correct and why? Give the answer number.\\nASSISTANT:\"\n",
    "        \n",
    "        ranking_response = self._generate_response(video_frames, prompt, max_new_tokens=100)\n",
    "        \n",
    "        # Extract the final answer index using regex\n",
    "        try:\n",
    "            answer_number_match = re.search(r'(\\d+)', ranking_response)\n",
    "            if answer_number_match:\n",
    "                answer_number = int(answer_number_match.group(1))\n",
    "                # Adjust to 0-based indexing\n",
    "                answer_index = answer_number - 1\n",
    "                if 0 <= answer_index < len(choices):\n",
    "                    final_answer = choices[answer_index]\n",
    "                else:\n",
    "                    final_answer = ranking_response  # Use full response if index out of range\n",
    "            else:\n",
    "                final_answer = ranking_response\n",
    "        except:\n",
    "            final_answer = ranking_response\n",
    "            \n",
    "        return final_answer, ranking_response, scores_and_rationales\n",
    "    \n",
    "    def step_5_answer_verification(self, video_frames, question, final_answer, action_analysis):\n",
    "        \"\"\"\n",
    "        Step 5: Answer Verification\n",
    "        \n",
    "        Args:\n",
    "            video_frames: Video frame tensors\n",
    "            question: The question text\n",
    "            final_answer: The final answer from step 4\n",
    "            action_analysis: The action analysis from step 3\n",
    "            \n",
    "        Returns:\n",
    "            Verification of the answer\n",
    "        \"\"\"\n",
    "        prompt = f\"USER: <video>\\nQuestion: {question}\\nSelected answer: {final_answer}\\n\\nBased on the video evidence and this analysis:\\n{action_analysis}\\n\\nVerify whether this answer is correct. Provide a final verdict (correct/incorrect) with justification.\\nASSISTANT:\"\n",
    "        \n",
    "        response = self._generate_response(video_frames, prompt, max_new_tokens=150)\n",
    "        return response\n",
    "    \n",
    "    def video_qa_reasoning(self, video_frames, question, choices, is_multi_choice=True, output_intermediate_steps=False):\n",
    "        \"\"\"\n",
    "        Complete video QA reasoning process using the Video-of-Thought approach\n",
    "        \n",
    "        Args:\n",
    "            video_frames: Video frame tensors\n",
    "            question: The question text\n",
    "            choices: List of answer choices\n",
    "            is_multi_choice: Whether the question is multiple choice\n",
    "            output_intermediate_steps: Whether to output intermediate reasoning steps\n",
    "            \n",
    "        Returns:\n",
    "            Final answer and optionally intermediate steps\n",
    "        \"\"\"\n",
    "        print(\"Step 1: Identifying targets...\")\n",
    "        targets = self.step_1_identify_targets(video_frames, question, is_multi_choice)\n",
    "\n",
    "        print('target:',targets)\n",
    "        \n",
    "        print(\"Step 2: Describing objects...\")\n",
    "        object_descriptions = self.step_2_object_description(video_frames, targets, question)\n",
    "\n",
    "        print('object_descriptions:',object_descriptions)\n",
    "        \n",
    "        print(\"Step 3: Analyzing actions...\")\n",
    "        action_analysis = self.step_3_action_analysis(video_frames, object_descriptions, question)\n",
    "\n",
    "        print('action_analysis:',action_analysis)\n",
    "        \n",
    "        print(\"Step 4: Scoring and ranking answers...\")\n",
    "        final_answer, ranking_response, scores = self.step_4_answer_scoring(\n",
    "            video_frames, question, choices, action_analysis\n",
    "        )\n",
    "\n",
    "        print('final_answer:',final_answer)\n",
    "        \n",
    "        print(\"Step 5: Verifying answer...\")\n",
    "        verification = self.step_5_answer_verification(\n",
    "            video_frames, question, final_answer, action_analysis\n",
    "        )\n",
    "\n",
    "        print('verification:',verification)\n",
    "        \n",
    "        # Format the final result\n",
    "        if is_multi_choice:\n",
    "            # Try to extract the answer index\n",
    "            answer_number = \"Unknown\"\n",
    "            try:\n",
    "                answer_number_match = re.search(r'(\\d+)', ranking_response)\n",
    "                if answer_number_match:\n",
    "                    answer_number = answer_number_match.group(1)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "            final_result = f\"Answer: {answer_number}\\nRationale: {verification}\"\n",
    "        else:\n",
    "            final_result = f\"Answer: {final_answer}\\nRationale: {verification}\"\n",
    "        \n",
    "        if output_intermediate_steps:\n",
    "            return {\n",
    "                \"targets\": targets,\n",
    "                \"object_descriptions\": object_descriptions,\n",
    "                \"action_analysis\": action_analysis,\n",
    "                \"scores\": scores,\n",
    "                \"ranking\": ranking_response,\n",
    "                \"final_answer\": final_answer,\n",
    "                \"verification\": verification,\n",
    "                \"final_result\": final_result\n",
    "            }\n",
    "        else:\n",
    "            return final_result\n",
    "    \n",
    "    def video_qa_direct(self, video_frames, question, choices=None, max_new_tokens=100):\n",
    "        \"\"\"\n",
    "        Standard video QA without the step-by-step reasoning process\n",
    "        \n",
    "        Args:\n",
    "            video_frames: Video frame tensors\n",
    "            question: The question text\n",
    "            choices: List of answer choices or None for open-ended questions\n",
    "            max_new_tokens: Maximum number of tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            Direct answer without step-by-step reasoning\n",
    "        \"\"\"\n",
    "        if choices:\n",
    "            # Format multiple-choice question\n",
    "            choice_with_idx = [f'\"{i+1}\": {choice}\\n' for i, choice in enumerate(choices)]\n",
    "            prompt = f\"USER: <video>\\n {question} \\n {choice_with_idx} Answer with the option's index from the given choices directly. \\n ASSISTANT: \"\n",
    "        else:\n",
    "            # Open-ended question\n",
    "            prompt = f\"USER: <video>\\n {question} \\n Answer directly based on what you see in the video. \\n ASSISTANT: \"\n",
    "        \n",
    "        response = self._generate_response(video_frames, prompt, max_new_tokens)\n",
    "        return response\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# video_llava_model = VideoQAModel(load_in_bits=4)\n",
    "# predictor = VideoOfThoughtPredictor(video_llava_model)\n",
    "# \n",
    "# # For multi-choice question:\n",
    "# result = predictor.video_qa_reasoning(\n",
    "#     video_frames=frames,\n",
    "#     question=\"What is the person doing in the video?\",\n",
    "#     choices=[\"Cooking\", \"Dancing\", \"Reading\", \"Exercising\"],\n",
    "#     output_intermediate_steps=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "501a54d4-3e00-4df6-8c27-e440742d841d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████| 3/3 [00:36<00:00, 12.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize with your VideoLLAVA model\n",
    "video_llava_model = VideoQAModel(load_in_bits=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12e530b-9041-43ad-a13d-685a5ca4981f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd71850d-4e0f-41a2-8682-772dc0350cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_pkl = \"/data/user_data/gdhanuka/STAR_dataset/STAR_val.pkl\"\n",
    "val_pkl = \"data/STAR_val.pkl\"\n",
    "# video_dir = \"/data/user_data/gdhanuka/STAR_dataset/Charades_v1_480\"\n",
    "video_dir = \"data/Charades_v1_480\"\n",
    "\n",
    "# File paths\n",
    "# results_file = \"/home/gdhanuka/STAR_Benchmark/analysis/video_llava_results2.jsonl\"\n",
    "results_file = \"analysis/video_llava_4_frames_results.jsonl\"\n",
    "# final_accuracy_file = \"/home/gdhanuka/STAR_Benchmark/analysis/video_llava_final_accuracy2.txt\"\n",
    "final_accuracy_file = \"analysis/video_llava_4_frames_final_accuracy.txt\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dataset = VideoQADataset(val_pkl, video_dir=video_dir, sampling_fps=4, num_frames=8, use_fps=False)\n",
    "# batched inference not working!!\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,  # disable to easily reproduce\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    # collate_fn=collate_fn,\n",
    ")\n",
    "category_correct = defaultdict(int)\n",
    "category_total = defaultdict(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1fad9da5-59e4-4ace-9a70-b592dbdbccea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f4be252d-ec46-498c-8c9f-0a946b0e623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "\n",
    "def display_video(video_id, video_dir=\"data/Charades_v1_480\"):\n",
    "    video_path = os.path.join(video_dir, f\"{video_id}.mp4\")\n",
    "    \n",
    "    html_content = f'''\n",
    "    <video width=\"320\" height=\"240\" controls>\n",
    "      <source src=\"{video_path}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    '''\n",
    "    \n",
    "    return HTML(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0dc899e7-7b71-4690-84c2-7d97207a8357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['video_frames', 'question', 'video_id', 'choices', 'answer_idx', 'category', 'all_text_inputs', 'data_proc_time', 'question_id', 'frame_ids'])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04c45330-fb42-45a2-abb3-8c6a81bde248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which object was tidied up by the person?\n",
      "[['The closet/cabinet.'], ['The blanket.'], ['The clothes.'], ['The table.']]\n"
     ]
    }
   ],
   "source": [
    "video_frames = batch[\"video_frames\"][0].to(device)\n",
    "question = batch[\"question\"][0]\n",
    "choices = batch[\"choices\"]\n",
    "\n",
    "print(question)\n",
    "print(choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7d2bf95-c046-4f83-9c54-be5727c3030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = VideoOfThoughtPredictor(video_llava_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "57057098-399b-4934-be5f-74c65dc932c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = batch['video_id'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7b421933-cbbc-47fa-b53f-85cfdd5a7637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <video width=\"320\" height=\"240\" controls>\n",
       "      <source src=\"data/Charades_v1_480/6H78U.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_video(video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fcf9c348-d65f-4d9b-b880-ebfd1ca5fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_qa(self, video_frames, question, choices, max_new_tokens=500):\n",
    "    choice_with_idx = [f'\"{i+1}\": {choice}\\n' for i, choice in enumerate(choices)]\n",
    "    # prompt = f\"USER: <video>\\n {question} \\n {choice_with_idx} Answer the question. Think step by step. \\n ASSISTANT: \"\n",
    "    prompt = \"USER: <video>Why is this video funny? ASSISTANT:\"\n",
    "    inputs = self.processor(\n",
    "        text=prompt, videos=video_frames, return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    ouputs = self.model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    print(ouputs)\n",
    "    \n",
    "    return ouputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c974488f-8d6b-4b70-b36b-64223693115e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  3148,  1001, 29901, 29871, 32001, 11008,   338,   445,  4863,\n",
      "          2090,  1460, 29973,   319,  1799,  9047, 13566, 29901,   450, 29892,\n",
      "           322, 29892,   322,   322,   322,   322,   322, 29973,   322,   322,\n",
      "           322,   322, 29892,   322, 29892,   322, 29892,   322, 29892,   518,\n",
      "           322, 29892,   518,   322, 29892,   518,   322, 29892,   322, 29892,\n",
      "           322, 29892,   322, 29892,   322, 29892,   322, 29892,   322, 29892,\n",
      "           322, 29892,   322, 29892,   322, 29892,   322, 29892,   322, 29892,\n",
      "           322, 29892,   322, 29892,   322, 29892,   322, 29892,   322, 29892,\n",
      "           322, 29892,   322, 29892,   322, 29892,   518,   322, 29892,   322,\n",
      "         29892,   322, 29892,   518,   322, 29892,   322, 29892,   322, 29892,\n",
      "           518,   322, 29892,   518,   322, 29892,   518,   322, 29892,   518,\n",
      "           322, 29892,   322, 29892,   322, 29892,   518,   518,   518,   518,\n",
      "           322, 29892,   322, 29892,   518,   322, 29892,   322, 29892,   322,\n",
      "         29892,   322, 29892,   322, 29889, 31147, 29889, 31147, 29892,   322,\n",
      "         29892,   322, 29892,   518,   322, 29892,   518,   322, 29892,   518,\n",
      "           322, 29892,   518, 29889,   518, 29889,   518, 29889,   518, 29889,\n",
      "           518, 29889,   322, 29892,   322, 29892,   322, 29892,   518,   322,\n",
      "         29892,   518,   322, 29892,   518, 29889,   518, 29889,   322, 29892,\n",
      "           518, 29889,   518, 29889,   322, 29892,   322, 29892,   322, 29889,\n",
      "           322, 29889,   322, 29892,   322, 29892,   322, 29892,   322, 29892,\n",
      "           322, 29892,   322, 29892,   322, 29892,   322, 29892,   322, 29892,\n",
      "           322, 29892,   322, 29892,   322, 29892,   322, 29892,   322, 29892,\n",
      "           322, 29892,   322, 29892,   322, 29892,   322, 29892,   322, 29892,\n",
      "           322, 29892,   322, 29892,   322, 29892,   322, 29892,   322, 29892,\n",
      "             2]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1,  3148,  1001, 29901, 29871, 32001, 11008,   338,   445,  4863,\n",
       "          2090,  1460, 29973,   319,  1799,  9047, 13566, 29901,   450, 29892,\n",
       "           322, 29892,   322,   322,   322,   322,   322, 29973,   322,   322,\n",
       "           322,   322, 29892,   322, 29892,   322, 29892,   322, 29892,   518,\n",
       "           322, 29892,   518,   322, 29892,   518,   322, 29892,   322, 29892,\n",
       "           322, 29892,   322, 29892,   322, 29892,   322, 29892,   322, 29892,\n",
       "           322, 29892,   322, 29892,   322, 29892,   322, 29892,   322, 29892,\n",
       "           322, 29892,   322, 29892,   322, 29892,   322, 29892,   322, 29892,\n",
       "           322, 29892,   322, 29892,   322, 29892,   518,   322, 29892,   322,\n",
       "         29892,   322, 29892,   518,   322, 29892,   322, 29892,   322, 29892,\n",
       "           518,   322, 29892,   518,   322, 29892,   518,   322, 29892,   518,\n",
       "           322, 29892,   322, 29892,   322, 29892,   518,   518,   518,   518,\n",
       "           322, 29892,   322, 29892,   518,   322, 29892,   322, 29892,   322,\n",
       "         29892,   322, 29892,   322, 29889, 31147, 29889, 31147, 29892,   322,\n",
       "         29892,   322, 29892,   518,   322, 29892,   518,   322, 29892,   518,\n",
       "           322, 29892,   518, 29889,   518, 29889,   518, 29889,   518, 29889,\n",
       "           518, 29889,   322, 29892,   322, 29892,   322, 29892,   518,   322,\n",
       "         29892,   518,   322, 29892,   518, 29889,   518, 29889,   322, 29892,\n",
       "           518, 29889,   518, 29889,   322, 29892,   322, 29892,   322, 29889,\n",
       "           322, 29889,   322, 29892,   322, 29892,   322, 29892,   322, 29892,\n",
       "           322, 29892,   322, 29892,   322, 29892,   322, 29892,   322, 29892,\n",
       "           322, 29892,   322, 29892,   322, 29892,   322, 29892,   322, 29892,\n",
       "           322, 29892,   322, 29892,   322, 29892,   322, 29892,   322, 29892,\n",
       "           322, 29892,   322, 29892,   322, 29892,   322, 29892,   322, 29892,\n",
       "             2]], device='cuda:0')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_qa(video_llava_model, video_frames, question, choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a463d72a-4030-4025-bc32-d04203fdf5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac3f7b-70c0-4f84-b168-e0f5f89216ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ed367f5-81a5-4e0a-b88d-beeff54750ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Identifying targets...\n",
      "tensor([[    1,  3148,  1001, 29901, 29871, 32001,    13,  3492,   526,   385,\n",
      "         17924,   297,  4863,  7418, 29889,    13,    13, 29954,  5428,   278,\n",
      "          1139, 29901,   376,  8809,   436,  1203,   471, 10668,  1000,   701,\n",
      "           491,   278,  2022, 29973,   613,   825,   526,   278,  1820,  3618,\n",
      "         29892,  2305, 29892,   470,  3161,   297,   278,  4863,   393,   817,\n",
      "           304,   367,  5702,   287,   304,  1234,   445,  1139, 29973,    13,\n",
      "            13,  1184, 29894,   680,   263,  3022,   895,  1051,   310,   278,\n",
      "          1820, 22525, 29889,    13, 22933,  9047, 13566, 29901,   450,   322,\n",
      "           322,   322,   322,   322,   322,     2]], device='cuda:0')\n",
      "target: The and and and and and and\n",
      "Step 2: Describing objects...\n",
      "tensor([[    1,  3148,  1001, 29901, 29871, 32001,    13,  4002, 29581,   297,\n",
      "          9493,   278,  1494,  3161,   393,   526,  8018,   304, 22862,   278,\n",
      "          1139,   376,  8809,   436,  1203,   471, 10668,  1000,   701,   491,\n",
      "           278,  2022, 29973,  1115,    13,    13,  1576,   322,   322,   322,\n",
      "           322,   322,   322,    13,    13, 20560,   373,  1009, 10097, 29892,\n",
      "         10298, 29892,   322, 22060,   297,   278,  4863, 29889,    13, 22933,\n",
      "          9047, 13566, 29901,   450, 29889, 31147, 29889,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0]], device='cuda:0')\n",
      "object_descriptions: The.Ъ.\n",
      "Step 3: Analyzing actions...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# For a multiple-choice question\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_qa_reasoning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchoices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_multi_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_intermediate_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to True to see all reasoning steps\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 216\u001b[0m, in \u001b[0;36mVideoOfThoughtPredictor.video_qa_reasoning\u001b[0;34m(self, video_frames, question, choices, is_multi_choice, output_intermediate_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_descriptions:\u001b[39m\u001b[38;5;124m'\u001b[39m,object_descriptions)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 3: Analyzing actions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m action_analysis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_3_action_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_descriptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_analysis:\u001b[39m\u001b[38;5;124m'\u001b[39m,action_analysis)\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 4: Scoring and ranking answers...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[40], line 120\u001b[0m, in \u001b[0;36mVideoOfThoughtPredictor.step_3_action_analysis\u001b[0;34m(self, video_frames, object_descriptions, question)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03mStep 3: Action Analysis\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    Analysis of actions and implications\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSER: <video>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBased on the question \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m and these observations:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mobject_descriptions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnalyze what actions are occurring in the video, their sequence, and their implications. Include both direct observations and reasonable inferences.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mASSISTANT:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 120\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "Cell \u001b[0;32mIn[40], line 41\u001b[0m, in \u001b[0;36mVideoOfThoughtPredictor._generate_response\u001b[0;34m(self, video_frames, prompt, max_new_tokens)\u001b[0m\n\u001b[1;32m     33\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mprocessor(\n\u001b[1;32m     34\u001b[0m     text\u001b[38;5;241m=\u001b[39mprompt, \n\u001b[1;32m     35\u001b[0m     videos\u001b[38;5;241m=\u001b[39mvideo_frames, \n\u001b[1;32m     36\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     37\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m\n\u001b[1;32m     38\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Use more controlled generation parameters\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# do_sample=False,  # Use greedy decoding for more consistent outputs\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# temperature=0.1,   # Lower temperature for more focused responses\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# num_beams=1,\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# early_stopping=True,\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# pad_token_id=self.model.processor.tokenizer.pad_token_id,\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# eos_token_id=self.model.processor.tokenizer.eos_token_id\u001b[39;49;00m\n\u001b[1;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs)\n\u001b[1;32m     54\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[1;32m     55\u001b[0m     outputs,\n\u001b[1;32m     56\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     57\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     58\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3210\u001b[0m     outputs,\n\u001b[1;32m   3211\u001b[0m     model_kwargs,\n\u001b[1;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3213\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/transformers/models/video_llava/modeling_video_llava.py:656\u001b[0m, in \u001b[0;36mVideoLlavaForConditionalGeneration.forward\u001b[0;34m(self, input_ids, pixel_values_images, pixel_values_videos, attention_mask, position_ids, past_key_values, inputs_embeds, vision_feature_layer, vision_feature_select_strategy, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep)\u001b[0m\n\u001b[1;32m    653\u001b[0m         video_features \u001b[38;5;241m=\u001b[39m video_features\u001b[38;5;241m.\u001b[39mto(inputs_embeds\u001b[38;5;241m.\u001b[39mdevice, inputs_embeds\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    654\u001b[0m         inputs_embeds \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39mmasked_scatter(special_image_mask, video_features)\n\u001b[0;32m--> 656\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    669\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    671\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:676\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 676\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    689\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:457\u001b[0m, in \u001b[0;36mLlamaFlashAttention2.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m--> 457\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:224\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    222\u001b[0m cos \u001b[38;5;241m=\u001b[39m cos\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    223\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[0;32m--> 224\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    225\u001b[0m k_embed \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(k) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "File \u001b[0;32m~/miniconda3/envs/V-LLAVA/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:198\u001b[0m, in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Rotates half the hidden dims of the input.\"\"\"\u001b[39;00m\n\u001b[1;32m    197\u001b[0m x1 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m--> 198\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;241m-\u001b[39mx2, x1), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# For a multiple-choice question\n",
    "result = predictor.video_qa_reasoning(\n",
    "    video_frames=video_frames,\n",
    "    question=question,\n",
    "    choices=choices,\n",
    "    is_multi_choice=True,\n",
    "    output_intermediate_steps=True  # Set to True to see all reasoning steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "244401ea-4d73-493c-93d3-1b83d88fc697",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# For a multiple-choice question\u001b[39;00m\n\u001b[1;32m      2\u001b[0m result \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mvideo_qa_reasoning(\n\u001b[0;32m----> 3\u001b[0m     video_frames\u001b[38;5;241m=\u001b[39m\u001b[43mframes\u001b[49m,\n\u001b[1;32m      4\u001b[0m     question\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the person doing in the video?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     choices\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCooking\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDancing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExercising\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m     is_multi_choice\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     output_intermediate_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Set to True to see all reasoning steps\u001b[39;00m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# For an open-ended question\u001b[39;00m\n\u001b[1;32m     11\u001b[0m result \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mvideo_qa_reasoning(\n\u001b[1;32m     12\u001b[0m     video_frames\u001b[38;5;241m=\u001b[39mframes,\n\u001b[1;32m     13\u001b[0m     question\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the person doing in the video?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     is_multi_choice\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'frames' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# For an open-ended question\n",
    "result = predictor.video_qa_reasoning(\n",
    "    video_frames=frames,\n",
    "    question=\"What is the person doing in the video?\",\n",
    "    is_multi_choice=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b20931-5aff-4c41-ac79-d8a99eb43ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (V-LLAVA)",
   "language": "python",
   "name": "v-llava"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
