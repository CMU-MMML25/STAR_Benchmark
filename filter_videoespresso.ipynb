{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "025a4393-135e-4457-bdaf-37b3af1a437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "074e022f-dd51-452b-9611-08d339dade10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jamesdin/miniconda3/envs/videollava/lib/python3.10/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-22 21:29:10,169] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jamesdin/miniconda3/envs/videollava/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/home/jamesdin/miniconda3/envs/videollava/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Placeholder paths - Replace with actual file paths\n",
    "VIDEO_EXPRESSO_DATA_PATH = '/home/jamesdin/MMML/STAR/videoespresso_train_video.json' # Path to the text file containing the Video Expresso markdown table\n",
    "STAR_ACTION_CLASSES_PATH = '/data/user_data/jamesdin/STAR/data/classes/action_classes.txt'\n",
    "STAR_OBJECT_CLASSES_PATH = '/data/user_data/jamesdin/STAR/data/classes/object_classes.txt'\n",
    "STAR_VERB_CLASSES_PATH = '/data/user_data/jamesdin/STAR/data/classes/verb_classes.txt'\n",
    "STAR_RELATIONSHIP_CLASSES_PATH = '/data/user_data/jamesdin/STAR/data/classes/relationship_classes.txt' # Optional, for more complex sentences\n",
    "\n",
    "OUTPUT_CSV_PATH = 'selected_star_similar_examples.csv' # Path to save the results\n",
    "\n",
    "NUM_STAR_SAMPLES_FOR_TARGET = 100\n",
    "\n",
    "\n",
    "# Model Selection:\n",
    "# 'all-mpnet-base-v2' -> High quality, slower\n",
    "# 'all-MiniLM-L6-v2'  -> Good quality, much faster, smaller\n",
    "MODEL_NAME = 'all-mpnet-base-v2'\n",
    "\n",
    "# Number of examples to select\n",
    "N_SELECT = 3 # Using 3 for demonstration, change as needed\n",
    "\n",
    "# Number of synthetic STAR sentences to generate for the target representation\n",
    "NUM_TARGET_SENTENCES = 100\n",
    "\n",
    "# Device for computation ('cuda' for GPU, 'cpu' for CPU)\n",
    "DEVICE = 'cuda' # Change to 'cpu' if no GPU is available\n",
    "\n",
    "# --- Helper Functions ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8b5aa62-af14-4a6f-bdc0-36decde40044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_class_list(filepath):\n",
    "    \"\"\"Loads class descriptions from a STAR dataset file.\"\"\"\n",
    "    classes = []\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                # Extract text after the class ID (e.g., \"a000 hold some clothes\" -> \"hold some clothes\")\n",
    "                match = re.match(r'^[a-z]\\d{3}\\s+(.*)', line.strip())\n",
    "                if match:\n",
    "                    classes.append(match.group(1))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}. Please provide the correct path.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {filepath}: {e}\")\n",
    "        return None\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ab818a72-e90a-4d08-a72d-981a806cfc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_class_map(filepath):\n",
    "    \"\"\"Loads STAR class IDs and descriptions into a dictionary.\"\"\"\n",
    "    class_map = {}\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(maxsplit=1)\n",
    "                if len(parts) == 2:\n",
    "                    # Use class ID as key, description as value\n",
    "                    class_map[parts[0]] = parts[1]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Class mapping file not found at {filepath}. Descriptions will be missing.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error reading class mapping file {filepath}: {e}\")\n",
    "    return class_map\n",
    "    \n",
    "def format_star_choices(choices_str):\n",
    "    \"\"\"Formats the 'choices' string into a readable sentence.\"\"\"\n",
    "    try:\n",
    "        choices_list = ast.literal_eval(choices_str)\n",
    "        if isinstance(choices_list, list):\n",
    "            choice_texts = [item.get('choice', '') for item in choices_list if isinstance(item, dict)]\n",
    "            return \"Choices are: \" + \"; \".join(filter(None, choice_texts)) + \".\"\n",
    "        return \"Choices could not be parsed.\"\n",
    "    except (ValueError, SyntaxError, TypeError):\n",
    "        # Handle cases where the string is not a valid list literal or structure is unexpected\n",
    "        # print(f\"Warning: Could not parse choices string: {choices_str}\")\n",
    "        return \"Choices format unclear.\"\n",
    "\n",
    "\n",
    "def format_star_situations(situations_str, action_map, object_map, relation_map):\n",
    "    \"\"\"Formats the 'situations' string into a readable paragraph.\"\"\"\n",
    "    situation_sentences = []\n",
    "    try:\n",
    "        situations_dict = ast.literal_eval(situations_str)\n",
    "        if not isinstance(situations_dict, dict):\n",
    "            return \"Situations format unclear.\"\n",
    "\n",
    "        # Aggregate information across all frames for simplicity, or process frame by frame\n",
    "        all_actions = set()\n",
    "        all_relations = []\n",
    "\n",
    "        for frame_id, frame_data in situations_dict.items():\n",
    "            if isinstance(frame_data, dict):\n",
    "                # Collect unique actions\n",
    "                for action_id in frame_data.get('actions',):\n",
    "                    all_actions.add(action_id)\n",
    "\n",
    "                # Collect unique relations\n",
    "                rel_pairs = frame_data.get('rel_pairs',)\n",
    "                rel_labels = frame_data.get('rel_labels',)\n",
    "                if len(rel_pairs) == len(rel_labels):\n",
    "                    for pair, label in zip(rel_pairs, rel_labels):\n",
    "                        if isinstance(pair, list) and len(pair) == 2:\n",
    "                            subj_id, obj_id = pair\n",
    "                            relation_tuple = (subj_id, label, obj_id)\n",
    "                            if relation_tuple not in all_relations:\n",
    "                                 all_relations.append(relation_tuple)\n",
    "\n",
    "        # Generate sentences for actions\n",
    "        person_text = object_map.get('o000', 'person') # Default subject\n",
    "        for action_id in all_actions:\n",
    "            action_text = action_map.get(action_id, action_id) # Use ID if mapping fails\n",
    "            situation_sentences.append(f\"{person_text.capitalize()} {action_text}.\")\n",
    "\n",
    "        # Generate sentences for relations\n",
    "        for subj_id, rel_id, obj_id in all_relations:\n",
    "             subj_text = object_map.get(subj_id, subj_id)\n",
    "             rel_text = relation_map.get(rel_id, rel_id)\n",
    "             obj_text = object_map.get(obj_id, obj_id)\n",
    "             # Handle cases like 'person on person' which might be less informative\n",
    "             if subj_text!= obj_text or subj_id!= 'o000': # Avoid self-relations unless specific\n",
    "                 situation_sentences.append(f\"{subj_text.capitalize()} {rel_text} {obj_text}.\")\n",
    "\n",
    "        if not situation_sentences:\n",
    "            return \"No specific situations described.\"\n",
    "\n",
    "        # Remove duplicates while preserving order (important for readability)\n",
    "        seen = set()\n",
    "        unique_sentences = [s for s in situation_sentences if not (s in seen or seen.add(s))]\n",
    "        return \"Situation: \" + \" \".join(unique_sentences)\n",
    "\n",
    "    except (ValueError, SyntaxError, TypeError):\n",
    "        # Handle cases where the string is not a valid dict literal or structure is unexpected\n",
    "        # print(f\"Warning: Could not parse situations string: {situations_str}\")\n",
    "        return \"Situations format unclear.\"\n",
    "\n",
    "\n",
    "def format_star_example(star_row, action_map, object_map, relation_map):\n",
    "    \"\"\"Combines question, answer, choices, and situations into one text block.\"\"\"\n",
    "    question = star_row.get('question', '')\n",
    "    answer = star_row.get('answer', '')\n",
    "    choices_str = star_row.get('choices', '')\n",
    "    situations_str = str(star_row.get('situations', '{}'))\n",
    "\n",
    "    formatted_choices = format_star_choices(choices_str)\n",
    "    formatted_situations = format_star_situations(situations_str, action_map, object_map, relation_map)\n",
    "\n",
    "    # Combine all parts\n",
    "    full_text = f\"Question: {question}\\nAnswer: {answer}\\n{formatted_choices}\\n{formatted_situations}\"\n",
    "    return full_text.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1db10df3-464e-48f0-957c-7e1f88fe3c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def synthesize_star_sentences(actions, objects, verbs, num_sentences=100):\n",
    "    \"\"\"Generates simple sentences representing STAR interactions.\"\"\"\n",
    "    sentences = []\n",
    "    if not actions and (not verbs or not objects):\n",
    "        print(\"Warning: Not enough class information to synthesize sentences.\")\n",
    "        return\n",
    "\n",
    "    # Strategy 1: Use full action descriptions if available\n",
    "    if actions:\n",
    "        sentences.extend([f\"A person {action}.\" for action in actions])\n",
    "\n",
    "    # Strategy 2: Combine verbs and objects\n",
    "    if verbs and objects:\n",
    "        for _ in range(num_sentences):\n",
    "            verb = random.choice(verbs)\n",
    "            obj = random.choice(objects)\n",
    "            # Basic sentence structure\n",
    "            sentence = f\"A person {verb} {obj}.\"\n",
    "            # Avoid adding exact duplicates immediately, though some overlap is fine\n",
    "            if sentence not in sentences:\n",
    "                 sentences.append(sentence)\n",
    "\n",
    "    # Ensure we have roughly the desired number, prioritizing unique combinations\n",
    "    if len(sentences) > num_sentences:\n",
    "        return random.sample(sentences, num_sentences)\n",
    "    elif len(sentences) < num_sentences and verbs and objects:\n",
    "         # Add more verb-object combinations if needed and possible\n",
    "         needed = num_sentences - len(sentences)\n",
    "         for _ in range(needed * 2): # Generate more to increase chance of new ones\n",
    "             verb = random.choice(verbs)\n",
    "             obj = random.choice(objects)\n",
    "             sentence = f\"A person {verb} {obj}.\"\n",
    "             if sentence not in sentences:\n",
    "                 sentences.append(sentence)\n",
    "             if len(sentences) >= num_sentences:\n",
    "                 break\n",
    "\n",
    "    # Final shuffle and trim if slightly over\n",
    "    random.shuffle(sentences)\n",
    "    return sentences[:num_sentences]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ba632a1c-b544-4b2b-b10a-616593348bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Video Expresso data from: /home/jamesdin/MMML/STAR/videoespresso_train_video.json\n",
      "Loaded 200766 examples from Video Expresso data.\n",
      "Loaded and processed 200766 examples from Video Expresso data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>key_item</th>\n",
       "      <th>evidence</th>\n",
       "      <th>task</th>\n",
       "      <th>video_path</th>\n",
       "      <th>keyframes_path</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Compare and contrast the subjects and the focu...</td>\n",
       "      <td>The first frame, at 203 seconds, focuses on a ...</td>\n",
       "      <td>[[bear], [flower, branch]]</td>\n",
       "      <td>The first frame highlights a &lt;obj_start&gt;bear i...</td>\n",
       "      <td>Narrative Analysis</td>\n",
       "      <td>Moviechat/videos/1/AWG-5.mp4</td>\n",
       "      <td>[Moviechat/videos/1_image/AWG-5/6100.jpg, Movi...</td>\n",
       "      <td>Compare and contrast the subjects and the focu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Connect George's daydream and subsequent slipp...</td>\n",
       "      <td>George's daydream about floating bananas, duri...</td>\n",
       "      <td>[[joy, banana], [dog, banana peel], [puddle, m...</td>\n",
       "      <td>George's daydream about a &lt;obj_start&gt;banana in...</td>\n",
       "      <td>Event Dynamic Analysis</td>\n",
       "      <td>Storystream/George/000277/output_videos/000277...</td>\n",
       "      <td>[Storystream/George/000277/000277_keyframe_0-2...</td>\n",
       "      <td>Connect George's daydream and subsequent slipp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How did George transition from his involvement...</td>\n",
       "      <td>George seems to transition from a contemplativ...</td>\n",
       "      <td>[[easel], [desert, rock formations]]</td>\n",
       "      <td>George's transition from the basin near the &lt;o...</td>\n",
       "      <td>Event Dynamic Analysis</td>\n",
       "      <td>Storystream/George/000140/output_videos/000140...</td>\n",
       "      <td>[Storystream/George/000140/000140_keyframe_0-1...</td>\n",
       "      <td>How did George transition from his involvement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Using the provided times, describe the steps i...</td>\n",
       "      <td>To prepare the mixture for Savoury Potato Panc...</td>\n",
       "      <td>[[mashed potatoes, bowl], [butter], [bowl, milk]]</td>\n",
       "      <td>The mixture for Savoury Potato Pancakes starts...</td>\n",
       "      <td>Preparation Steps / Ingredient Analysis</td>\n",
       "      <td>Youcook2/merged/2/N35UyfIwhVI.mp4</td>\n",
       "      <td>[Youcook2/merged/2_image/N35UyfIwhVI/525.jpg, ...</td>\n",
       "      <td>Using the provided times, describe the steps i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do the two nocturnal foraging behaviors of...</td>\n",
       "      <td>The raccoons are depicted foraging on the grou...</td>\n",
       "      <td>[[environment, raccoons], [echolocation, bats]]</td>\n",
       "      <td>The &lt;obj_start&gt;raccoons in frame 1&lt;obj_end&gt;&lt;bo...</td>\n",
       "      <td>Causal Analysis</td>\n",
       "      <td>Moviechat/videos/4/BWB-4.mp4</td>\n",
       "      <td>[Moviechat/videos/4_image/BWB-4/9850.jpg, Movi...</td>\n",
       "      <td>How do the two nocturnal foraging behaviors of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Compare and contrast the subjects and the focu...   \n",
       "1  Connect George's daydream and subsequent slipp...   \n",
       "2  How did George transition from his involvement...   \n",
       "3  Using the provided times, describe the steps i...   \n",
       "4  How do the two nocturnal foraging behaviors of...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The first frame, at 203 seconds, focuses on a ...   \n",
       "1  George's daydream about floating bananas, duri...   \n",
       "2  George seems to transition from a contemplativ...   \n",
       "3  To prepare the mixture for Savoury Potato Panc...   \n",
       "4  The raccoons are depicted foraging on the grou...   \n",
       "\n",
       "                                            key_item  \\\n",
       "0                         [[bear], [flower, branch]]   \n",
       "1  [[joy, banana], [dog, banana peel], [puddle, m...   \n",
       "2               [[easel], [desert, rock formations]]   \n",
       "3  [[mashed potatoes, bowl], [butter], [bowl, milk]]   \n",
       "4    [[environment, raccoons], [echolocation, bats]]   \n",
       "\n",
       "                                            evidence  \\\n",
       "0  The first frame highlights a <obj_start>bear i...   \n",
       "1  George's daydream about a <obj_start>banana in...   \n",
       "2  George's transition from the basin near the <o...   \n",
       "3  The mixture for Savoury Potato Pancakes starts...   \n",
       "4  The <obj_start>raccoons in frame 1<obj_end><bo...   \n",
       "\n",
       "                                      task  \\\n",
       "0                       Narrative Analysis   \n",
       "1                   Event Dynamic Analysis   \n",
       "2                   Event Dynamic Analysis   \n",
       "3  Preparation Steps / Ingredient Analysis   \n",
       "4                          Causal Analysis   \n",
       "\n",
       "                                          video_path  \\\n",
       "0                       Moviechat/videos/1/AWG-5.mp4   \n",
       "1  Storystream/George/000277/output_videos/000277...   \n",
       "2  Storystream/George/000140/output_videos/000140...   \n",
       "3                  Youcook2/merged/2/N35UyfIwhVI.mp4   \n",
       "4                       Moviechat/videos/4/BWB-4.mp4   \n",
       "\n",
       "                                      keyframes_path  \\\n",
       "0  [Moviechat/videos/1_image/AWG-5/6100.jpg, Movi...   \n",
       "1  [Storystream/George/000277/000277_keyframe_0-2...   \n",
       "2  [Storystream/George/000140/000140_keyframe_0-1...   \n",
       "3  [Youcook2/merged/2_image/N35UyfIwhVI/525.jpg, ...   \n",
       "4  [Moviechat/videos/4_image/BWB-4/9850.jpg, Movi...   \n",
       "\n",
       "                                       combined_text  \n",
       "0  Compare and contrast the subjects and the focu...  \n",
       "1  Connect George's daydream and subsequent slipp...  \n",
       "2  How did George transition from his involvement...  \n",
       "3  Using the provided times, describe the steps i...  \n",
       "4  How do the two nocturnal foraging behaviors of...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "\n",
    "# 1. Load Video Expresso Data\n",
    "print(f\"Loading Video Expresso data from: {VIDEO_EXPRESSO_DATA_PATH}\")\n",
    "\n",
    "with open(VIDEO_EXPRESSO_DATA_PATH, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    df_expresso = pd.DataFrame(data)\n",
    "\n",
    "print(f\"Loaded {len(df_expresso)} examples from Video Expresso data.\")\n",
    "\n",
    "df_expresso['combined_text'] = df_expresso['question'] + \"\\nAnswer: \" + df_expresso['answer']\n",
    "video_expresso_texts = df_expresso['combined_text'].tolist()\n",
    "print(f\"Loaded and processed {len(df_expresso)} examples from Video Expresso data.\")\n",
    "\n",
    "df_expresso.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "70ed33c7-6a71-4bc9-927e-e0c9375300dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading STAR dataset and class mappings...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'r000': 'on',\n",
       " 'r001': 'behind',\n",
       " 'r002': 'in_front_of',\n",
       " 'r003': 'on_the_side_of',\n",
       " 'r004': 'above',\n",
       " 'r005': 'beneath',\n",
       " 'r006': 'drinking_from',\n",
       " 'r007': 'have_it_on_the_back',\n",
       " 'r008': 'wearing',\n",
       " 'r009': 'holding',\n",
       " 'r010': 'lying_on',\n",
       " 'r011': 'covered_by',\n",
       " 'r012': 'carrying',\n",
       " 'r013': 'eating',\n",
       " 'r014': 'leaning_on',\n",
       " 'r015': 'sitting_on',\n",
       " 'r016': 'twisting',\n",
       " 'r017': 'writing_on',\n",
       " 'r018': 'standing_on',\n",
       " 'r019': 'touching',\n",
       " 'r020': 'wiping',\n",
       " 'r021': 'at',\n",
       " 'r022': 'under',\n",
       " 'r023': 'near'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 2. Load STAR Data and Class Mappings\n",
    "print(\"Loading STAR dataset and class mappings...\")\n",
    "\n",
    "import pickle\n",
    "# Load the .pkl file\n",
    "with open('/data/user_data/jamesdin/STAR/data/STAR_val.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "# Convert to DataFrame and set index\n",
    "df_star = pd.DataFrame(data).set_index('question_id')\n",
    "\n",
    "action_map = load_class_map(STAR_ACTION_CLASSES_PATH)\n",
    "object_map = load_class_map(STAR_OBJECT_CLASSES_PATH)\n",
    "relation_map = load_class_map(STAR_RELATIONSHIP_CLASSES_PATH)\n",
    "\n",
    "relation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6892245a-e1cf-4506-b687-055e2076d03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting and formatting 100 STAR examples for target representation...\n",
      "Formatted 100 STAR examples.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Question: Which object did the person throw before they held the phone/camera?\\nAnswer: The clothes.\\nChoices format unclear.\\nSituation: Person throw clothes somewhere. Person hold a phone/camera. Person holding clothes. Person in_front_of clothes. Person standing_on floor. Person beneath floor. Person touching clothes. Person holding phone/camera. Person in_front_of phone/camera.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# 3. Select and Format STAR Examples for Target Representation\n",
    "print(f\"Selecting and formatting {NUM_STAR_SAMPLES_FOR_TARGET} STAR examples for target representation...\")\n",
    "if len(df_star) < NUM_STAR_SAMPLES_FOR_TARGET:\n",
    "    print(f\"Warning: Requested {NUM_STAR_SAMPLES_FOR_TARGET} STAR samples, but only {len(df_star)} available. Using all available.\")\n",
    "    star_samples = df_star\n",
    "else:\n",
    "    star_samples = df_star.sample(n=NUM_STAR_SAMPLES_FOR_TARGET, random_state=42) # Use random_state for reproducibility\n",
    "\n",
    "target_texts = [format_star_example(row, action_map, object_map, relation_map) for _, row in star_samples.iterrows()]\n",
    "print(f\"Formatted {len(target_texts)} STAR examples.\")\n",
    "# print(\"\\nSample Formatted STAR Example:\")\n",
    "# print(target_texts) # Optional: view a sample\n",
    "\n",
    "target_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06688c73-9e02-44c1-b9d6-67db8e73ee6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0635a70b39b44aa870ca526de825208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490e1614c58a450a9c1ec63c8522ccc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8654e1cc2f914a5d90096be0f2b5ee40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e6962853a24fceb28c44d382f6e95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1f05a2c563439ab65380be96aa6a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1566c30be74a4979883ec9e47f778294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a6b27b0b1244209e9875bb9ad5473c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806b2215666d4263be0b5747491bacb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62dbe0d3a91645129ceaea0ad48a9d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d077ec3a8b194990a76a0d61e79b08c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6babf238db894dcaab5486a783f954ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer(MODEL_NAME, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1dac2307-436a-41fa-a9d3-0af50050e136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded693e6573249e0b8d92cf9671370ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "target_embeddings = model.encode(\n",
    "    target_texts,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=128 # Adjust based on GPU memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65260680-4232-41ba-8026-7937beef986b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b274e0014e48ccac9a95b7223b9ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 47.43 GiB of which 28.56 MiB is free. Process 1149652 has 11.73 GiB memory in use. Process 1149686 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 23.08 GiB memory in use. Of the allocated memory 22.74 GiB is allocated by PyTorch, and 34.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m video_expresso_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_expresso_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Adjust based on GPU memory\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/videollava/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:557\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 557\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    559\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m~/miniconda3/envs/videollava/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:624\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m    623\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/videollava/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/videollava/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/videollava/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:442\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[1;32m    436\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    437\u001b[0m     key: value\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    440\u001b[0m }\n\u001b[0;32m--> 442\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    443\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/videollava/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/videollava/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/videollava/lib/python3.10/site-packages/transformers/models/mpnet/modeling_mpnet.py:545\u001b[0m, in \u001b[0;36mMPNetModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m extended_attention_mask: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)\n\u001b[1;32m    544\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 545\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    547\u001b[0m     embedding_output,\n\u001b[1;32m    548\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    552\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    553\u001b[0m )\n\u001b[1;32m    554\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/videollava/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/videollava/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/videollava/lib/python3.10/site-packages/transformers/models/mpnet/modeling_mpnet.py:106\u001b[0m, in \u001b[0;36mMPNetEmbeddings.forward\u001b[0;34m(self, input_ids, position_ids, inputs_embeds, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings(input_ids)\n\u001b[1;32m    104\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[0;32m--> 106\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\n\u001b[1;32m    107\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)\n\u001b[1;32m    108\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacity of 47.43 GiB of which 28.56 MiB is free. Process 1149652 has 11.73 GiB memory in use. Process 1149686 has 12.58 GiB memory in use. Including non-PyTorch memory, this process has 23.08 GiB memory in use. Of the allocated memory 22.74 GiB is allocated by PyTorch, and 34.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "video_expresso_embeddings = model.encode(\n",
    "    video_expresso_texts,\n",
    "    normalize_embeddings=True,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32 # Adjust based on GPU memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f1d3a00d-58fa-4434-b69f-b087b475d2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating average STAR target embedding...\n",
      "Average STAR target embedding generated.\n"
     ]
    }
   ],
   "source": [
    " # 6. Calculate Average Target Embedding\n",
    "print(\"Calculating average STAR target embedding...\")\n",
    "if target_embeddings.shape[0] > 0:\n",
    "    target_embedding_avg = np.mean(target_embeddings, axis=0)\n",
    "    norm = np.linalg.norm(target_embedding_avg)\n",
    "    if norm > 0:\n",
    "         target_embedding_avg = target_embedding_avg / norm\n",
    "    else:\n",
    "        print(\"Warning: Average target embedding has zero norm.\")\n",
    "        target_embedding_avg = np.zeros(model.get_sentence_embedding_dimension())\n",
    "\n",
    "    target_embedding_avg_2d = target_embedding_avg.reshape(1, -1)\n",
    "    print(\"Average STAR target embedding generated.\")\n",
    "else:\n",
    "    print(\"Error: No target STAR embeddings were generated.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d7f5cb7f-f4a2-4111-a343-397aa2744d07",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video_expresso_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compare Video Expresso embeddings against the average STAR target embedding\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m similarity_scores_matrix \u001b[38;5;241m=\u001b[39m cosine_similarity(target_embedding_avg_2d, \u001b[43mvideo_expresso_embeddings\u001b[49m)\n\u001b[1;32m      3\u001b[0m similarity_scores \u001b[38;5;241m=\u001b[39m similarity_scores_matrix\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'video_expresso_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# Compare Video Expresso embeddings against the average STAR target embedding\n",
    "similarity_scores_matrix = cosine_similarity(target_embedding_avg_2d, video_expresso_embeddings)\n",
    "similarity_scores = similarity_scores_matrix.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d738a3e-db0d-4ecc-9e50-79a07329d1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8. Rank and Select Video Expresso Examples\n",
    "print(f\"Ranking Video Expresso examples and selecting top {N_SELECT}...\")\n",
    "df_expresso['similarity_to_star'] = similarity_scores\n",
    "df_ranked = df_expresso.sort_values(by='similarity_to_star', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Select the original columns plus the new similarity score for the output\n",
    "output_columns = [col for col in df_expresso.columns if col not in ['combined_text', 'similarity_to_star']] + ['similarity_to_star']\n",
    "selected_examples_df = df_ranked.head(N_SELECT)[output_columns]\n",
    "\n",
    "\n",
    "print(f\"\\n--- Selected Top {N_SELECT} Video Expresso Examples (Most Similar to STAR Samples) ---\")\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):\n",
    "    print(selected_examples_df)\n",
    "\n",
    "# 9. Save Results (Optional)\n",
    "try:\n",
    "    selected_examples_df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "    print(f\"\\nSelected examples saved to: {OUTPUT_CSV_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving results to CSV: {e}\")\n",
    "\n",
    "print(\"\\nProcess completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3472a2a-2d9a-4c25-8b57-20bba9a2d31c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (videollava)",
   "language": "python",
   "name": "videollava"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
